YOLOv1 2016 Joseph Redmon:
50M параметров
Проигрывала топовым моделям, но работала в 45 фпс (Faster R-CNN 17 фпс)
В основе GoogleNet
Выходной тензор 7*7*30, так как изображение делилось на 7*7 ячеек, внутри каждой ячейки 2 ббокса, вероятности 20 классов
плохо детектирует маленькие объекты и объекты в одной клетке (низкий recall)
Для определения класса перемножали вероятности и confidence score(вероятность того, что центр в этой ячейке, тоесть IoU)
Для выбора того, какие ббоксы оставить NMS (см в статье)



YOLOv2 и YOLO9000 december 2016:
замена полносвязных слоев на полносверточные в голове детектора -> картинки разного размера
Добавления anchor'ов, стали предсказывать смещения относительно центра, а не просто координаты, предсказание класса каждого анкора
Смена GoogleNet на более мелкую Darknet19 и получение на выходе тензора 13x13
В обучении стали увчитываться box loss и class loss
Вывод: обучение модели задачам детекции (на датасете COCO) и классификации (ImageNet) одновременно.
Выше скорость и точность

добавили batch norm, убрали dropout
размер изображения с 448 до 416,
теперь вероятность предсказывается не для всех ббоксов, а для каждого по отдельности
добавили skip-connection со слоя 26*26, для распознования маленьких объектов (делился на 4 области и конкатенировался)
Теперь анкер боксы, их 5 (их расчет см в статье)
Обучали на разных размерах изображений
Теперь может предсказывать 9000 классов, объединили COCO и imageNet при помощи wordTree
Значительно увеличился скор



YOLOv3 2018:
Теперь Darknet-53, у головы 3 выхода, для разных размеров, добавили мультиклассификацию
Добавили шею (см схему в статье)
!Новый лосс (см в статье)


YOLOv4 2020:
Смена команды
Основа - SCPDarknet-53

Weighted-Residual-Connections (WRC) - residual connections с весами, которые обучаются
Cross Stage Partial connections (CSP) - выход блока разделяется на части, где одна проходит через следующий болок, а другая пропускает следующий блок
Cross mini-Batch Normalization (CmBN) - матожидание и дисперсия вычисляются не только по текущему батчку, а используют еще другие (профит при обучении на несколких устрйоствах)
Self-adversarial training (SAT) - ухудшение входа => ухудшение собсвтенного предикта
Mish activation - x * tanh(ln(1 + e^x)) - гладкая ReLU с "ямкой" слева от нуля
Mosaic data augmentation    - ..
copy_paste augmentation
DropBlock regularization    - dropout для целых блоков
CIoU loss   - учитывает не только пересечение но и центрирование и соотношение сторон


YOLOv5 2020 Ultralytics :
Переход на PyTorch
Добавление n, s, m, l, x версий

Exponential Moving Average (EMA) - усреднение параметров за последние несколько итераций
Mixed Precision Training - f32 -> f16

- Mosaic augmentation;
- Аффинные преобразования и HSV;
- Горизонтальные флипы;
- MixUp;

- Замена CSP блока на С3 блок - В C3 блоках используются три свертки (Conv3) и добавлены элементы нормализации и активации для повышения производительности и стабильности обучения.
- Замена промежуточных блоков на С3 блоки -
- Замена SPP блока на SPPF - В SPPF блоке используются оптимизированные методы пула и свертки для ускорения обработки данных без потери точности.
- Замена функций активации во всей сети на [silu](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html) (в некоторых источниках называется swish).
